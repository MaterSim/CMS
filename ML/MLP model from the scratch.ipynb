{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron from the scratch\n",
    "\n",
    "Nowadays, multi-Layer neural network has been proved to be a powerful tool in many data science problems. Though many existing packages have provided the interfaces to call this function (e.g. scikit-learn), it would be good to write some toy model by your own. Through this practice, you will gain some experience in software engineering. More importantly, you will understand the underlying mathmatics better and know how to fix the troubles when you run the code from the existing softwares. In the tutorial, we will continue to use the wine data and figure out how to write our own MLP classfier.\n",
    "\n",
    "Let us start with the example in the previous lecture\n",
    "```\n",
    "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(4, 2), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
    "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
    "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
    "```\n",
    "<img src=\"img/MLP.jpeg\" style=\"width: 800px;\"/>\n",
    "<center> Figure 1, the MLP model used in this lecture</center>\n",
    "\n",
    "You should be able to understand most of the parameters at the moment. To realize a minimum version of MLP, we can try to implement the following parameters into our model:\n",
    "- hidden_layer_sizes: to make life easier, let us just consider 2 hidden layer models\n",
    "- max_iter: maximum number of iteractions\n",
    "- learning_rate_init: \n",
    "\n",
    "Note that we will completely ignore the terms related to regularization\n",
    "\n",
    "## Back propagation\n",
    "$$\\frac{\\partial L}{\\partial y} = y-Y$$\n",
    "$$\\frac{\\partial y}{\\partial f_3} = $$\n",
    "$$\\frac{\\partial f_3}{\\partial h_2} = $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_MLPClassifier(object):\n",
    "    \"\"\"\n",
    "    Basic MultiLayer Perceptron (MLP) neural network.\n",
    "    Args:\n",
    "    hidden layer: []\n",
    "    max_iterations: []\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden=[4,4], max_iterations = 50, learning_rate = 0.01, \n",
    "                decay_rate = 0.99, activation_method='sigmoid', loss_method='mse'):\n",
    "        \"\"\"\n",
    "        :param hidden: number of hidden neurons\n",
    "        :param iterations: how many epochs\n",
    "        :param learning_rate: initial learning rate\n",
    "        \"\"\"\n",
    "        # initialize input parameters\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.rate_decay = rate_decay\n",
    "        self.n_hid1, self.n_hid2 = hidden[0], hidden[1]\n",
    "        self.activation_method = activation_method\n",
    "        self.loss_method= loss_method\n",
    "        \n",
    "    def fit(self, input, target):      \n",
    "        \"\"\"\n",
    "        input: 2D array, N_sample * N_attributes\n",
    "        output: 1D array, N_sample * 1\n",
    "        \"\"\"\n",
    "        # Initialize the weights and bias according to the input/target data\n",
    "        dim_in, dim_out = input.shape[1], output.shape[1]\n",
    "        self.w1 = np.random.randn(dim_in, self.n_hid1)\n",
    "        self.w2 = np.random.randn(self.n_hid1, self.n_hid2)\n",
    "        self.w3 = np.random.randn(self.n_hid2, dim_out)\n",
    "        self.b1 = np.random.randn(1,self.n_hid1)\n",
    "        self.b2 = np.random.randn(1,self.n_hid2)\n",
    "        self.b3 = np.random.randn(1,dim_out) \n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            # forward function\n",
    "            (h1, h2, y) = self.forward(input)\n",
    "            \n",
    "            # evaluate and print the loss function    \n",
    "            loss = self.loss(y, target)\n",
    "                \n",
    "            # backpropagation\n",
    "            #g_loss = self.grad_loss(y, target)\n",
    "            #g_y \n",
    "            #gw3\n",
    "            #gb3\n",
    "            #gh2\n",
    "            #gw2\n",
    "            #gb2\n",
    "            #gh1\n",
    "            #gw1\n",
    "            #gb1\n",
    "            \n",
    "            # updating the weight\n",
    "            learning_rate = self.learning_rate * (self.decay_rate**i)\n",
    "            w3 -= learning_rate*gw3\n",
    "            b3 -= learning_rate*gb3\n",
    "            w2 -= learning_rate*gw2\n",
    "            b2 -= learning_rate*gb2\n",
    "            w1 -= learning_rate*gw1\n",
    "            b1 -= learning_rate*db1\n",
    "        self.weights = (w1, w2, w3)\n",
    "        self.bias = (b1, b2, b3)\n",
    "        loss = self.loss(y, target)\n",
    "        print('The final loss is {:12.4f} after {:4d} iterations'.format(self.loss, self.iterations))\n",
    "    \n",
    "    def loss(self, y, target):\n",
    "        if self.loss_method=='mse':\n",
    "            return 0.5*np.sum(np.power(y-target,2))\n",
    "        elif self.loss.method=='log_loss':\n",
    "            pass\n",
    "        else:\n",
    "            raise Notimplementederror\n",
    "    \n",
    "    def grad_loss(self, y, target):\n",
    "        if self.loss_method=='mse':\n",
    "            return y-target\n",
    "        elif self.loss_method=='log_loss':\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def activation(self, x, method='sigmoid'):\n",
    "        if self.activation_method=='sigmoid':\n",
    "            return 1./(1+np.exp(-x))\n",
    "        elif self.activation_method=='log_loss':\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    \n",
    "    def grad_activation(self, x, method='sigmoid'):\n",
    "        if self.activation_method=='sigmoid':\n",
    "            return x*(1-x)\n",
    "        elif self.activation_method=='log_loss':\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "    def forward(self, X)\n",
    "        h1 = self.activation(w1.dot(input)+b1)\n",
    "        h2 = self.activation(w2.dot(input)+b2)\n",
    "        y = self.activation(w3.dot(input)+b3)\n",
    "        \n",
    "        return (h1, h2, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        return predictions after training algorithm\n",
    "        \"\"\"\n",
    "        (h1, h2, y) = self.forward(X)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain and preprocess the data\n",
    "from sklearn.datasets import load_wine\n",
    "data=load_wine()\n",
    "x, Y = data.data, data.target\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(x)  \n",
    "x0 = scaler.transform(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mlp = my_MLPClassifier(hidden=[4,4])\n",
    "my_mlp.fit(x0, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
