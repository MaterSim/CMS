{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron from the scratch\n",
    "\n",
    "Nowadays, multi-Layer neural network has been proved to be a powerful tool in many data science problems. Though many existing packages have provided the interfaces to call this function (e.g. scikit-learn), it would be good to write some toy model by your own. Through this practice, you will gain some experience in software engineering. More importantly, you will understand the underlying mathmatics better and know how to fix the troubles when you run the code from the existing softwares. In the tutorial, we will continue to use the wine data and figure out how to write our own MLP classfier.\n",
    "\n",
    "Let us start with the example in the previous lecture\n",
    "```\n",
    "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(4, 4), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
    "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
    "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
    "```\n",
    "<img src=\"img/MLP.jpeg\" style=\"width: 800px;\"/>\n",
    "<center> Figure 1, the MLP model used in this lecture</center>\n",
    "\n",
    "You should be able to understand most of the parameters at the moment. To realize a minimum version of MLP, we can try to implement the following parameters into our model:\n",
    "- hidden_layer_sizes: to make life easier, let us just consider 2 hidden layer models\n",
    "- max_iter: maximum number of iteractions\n",
    "- learning_rate_init: \n",
    "\n",
    "Note that we will completely ignore the terms related to regularization. \n",
    "We also need to choose the proper loss and activation functions. For simplicity, let's choose the followings,\n",
    "- Activation: Sigmoid\n",
    "- Loss: logloss\n",
    "\n",
    "## Forward propagation\n",
    "$\n",
    "\\begin{align}\n",
    "h_1 &= w_1\\cdot x + b_1\\\\\n",
    "z_1 & = f(h_1)\\\\\n",
    "h_2 &= w_2\\cdot f_1(w_1\\cdot x + b_1)) + b_2 = w_2\\cdot z_1 + b_2\\\\\n",
    "z_2 & = f(h_2)\\\\\n",
    "h_3 &= w_3\\cdot f_2((w_2\\cdot f_1(w_1\\cdot x + b_1)) + b_2)) + b_3 = w_3\\cdot z_2 + b_3\\\\\n",
    "y` &= f_3(h_3) \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "## Loss function (log loss): \n",
    "\n",
    "Logloss function is a popular choice for classification problem. In this scheme, the target values represented by an array of values betwen 0 and 1 (e.g., y=[0, 0, 1], y\\`=[0.1, 0.1, 0.9]))\n",
    "\\begin{align}\n",
    "L = \\sum y\\log y` \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "## Back propagation\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "%\\frac{\\partial L}{\\partial y} & = y-Y \\\\\n",
    "\\Delta_3 & = \\frac{\\partial L}{\\partial z3} = \\frac{\\partial L}{\\partial y`} \\frac{\\partial y`}{\\partial z_3} = y`-Y \\\\\n",
    "\\frac{\\partial L}{\\partial w_3}& = \\Delta_3 \\cdot h_2,~~~\n",
    "\\frac{\\partial L}{\\partial b_3} = \\Delta_3, ~~~ \n",
    "\\frac{\\partial L}{\\partial h_2} = \\Delta_3 \\cdot w_3 \\\\\n",
    "\\Delta_2 & = \\frac{\\partial L}{\\partial h_2} \\frac{\\partial h_2}{\\partial z_2} = \\Delta_3 \\cdot w_3 (1-h_2)h_2\\\\\n",
    "\\frac{\\partial L}{\\partial w_2} & = \\Delta_2 h_1,~~~\n",
    "\\frac{\\partial L}{\\partial b_2} = \\Delta_2 ~~~\n",
    "\\frac{\\partial L}{\\partial h_1} = \\Delta_3 \\cdot w_2\\\\\n",
    "\\Delta_1 & = \\frac{\\partial L}{\\partial h_1} \\frac{\\partial h_1}{\\partial z_1} = \\Delta_2 \\cdot w_2 (1-h_1)h_1\\\\\n",
    "\\frac{\\partial L}{\\partial w_2} & = \\Delta_1 \\cdot x,~~~\n",
    "\\frac{\\partial L}{\\partial b_2} = \\Delta_1 \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class my_MLPClassifier(object):\n",
    "    \"\"\"\n",
    "    Basic MultiLayer Perceptron (MLP) neural network.\n",
    "    Args:\n",
    "    hidden layer: []\n",
    "    max_iterations: []\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden, max_iter, \n",
    "                 learning_rate=0.001, decay_rate=0.999, \n",
    "                 activation='sigmoid', loss='logloss'):\n",
    "        \"\"\"\n",
    "        :param hidden: number of hidden neurons\n",
    "        :param iterations: how many epochs\n",
    "        :param learning_rate: initial learning rate\n",
    "        \"\"\"\n",
    "        # initialize input parameters\n",
    "        self.iterations = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.n_hid1, self.n_hid2 = hidden[0], hidden[1]\n",
    "        self.activation_method = activation\n",
    "        self.loss_method = loss\n",
    "        \n",
    "    def fit(self, x, y):      \n",
    "        \"\"\"\n",
    "        input: [178, 13] = [example, wine features]\n",
    "        output: [178, 3] = [example, class probabilities] \n",
    "        \"\"\"\n",
    "        # Initialize the weights and bias according to the input/target data\n",
    "        dim_in, dim_out = x.shape[1], 3\n",
    "        self.w1 = np.random.randn(dim_in, self.n_hid1) # [13,4] -- 52 weights \n",
    "        self.w2 = np.random.randn(self.n_hid1, self.n_hid2) # [4,4] -- 16 weights\n",
    "        self.w3 = np.random.randn(self.n_hid2, dim_out) # [4,3] -- 12 weights\n",
    "        self.b1 = np.random.randn(1,self.n_hid1) # [1,4] -- 4 biases\n",
    "        self.b2 = np.random.randn(1,self.n_hid2) # [1,4] -- 4 biases\n",
    "        self.b3 = np.random.randn(1,dim_out)  # [1,3] -- 3 bias\n",
    "        loss_hist = [] # track loss for printing\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            # forward\n",
    "            (h1,h2,y_) = self.forward(x)\n",
    "            cost = self.loss_function(y_,y) \n",
    "            loss_hist.append(cost)\n",
    "            \n",
    "            # backprop\n",
    "            delta3 = (y_-y)\n",
    "            gradw3 = np.dot(np.transpose(h2),delta3)\n",
    "            gradb3 = np.sum(delta3, axis = 0)\n",
    "            delta2 = np.dot(delta3, np.transpose(self.w3)) * self.grad_activation(h2)\n",
    "            gradw2 = np.dot(np.transpose(h1),delta2)\n",
    "            gradb2 = np.sum(delta2, axis=0)  \n",
    "            delta1 = np.dot(delta2, np.transpose(self.w2)) * self.grad_activation(h1)\n",
    "            gradw1 = np.dot(np.transpose(x),delta1)\n",
    "            gradb1 = np.sum(delta1, axis=0)\n",
    "            \n",
    "            # update parameters\n",
    "            learning_rate = self.learning_rate * (self.decay_rate**i)\n",
    "            self.w3 -= learning_rate*gradw3\n",
    "            self.b3 -= learning_rate*gradb3\n",
    "            self.w2 -= learning_rate*gradw2\n",
    "            self.b2 -= learning_rate*gradb2\n",
    "            self.w1 -= learning_rate*gradw1\n",
    "            self.b1 -= learning_rate*gradb1\n",
    "            if i%100==0 or i+1==self.iterations:\n",
    "                print('activation function: {:s}, loss function: {:s} = {:8.3f}, iterations: {:d}, learning rate: {:6.4f}'\n",
    "                .format(self.activation_method, self.loss_method, round(cost,3), i, learning_rate))\n",
    "        self.weights = (self.w3, self.w2, self.w1)\n",
    "        self.bias = (self.b3, self.b2, self.b1)\n",
    "        \n",
    "        # plot the training curve\n",
    "        plt.title('training curve')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.plot(np.arange(0,self.iterations),loss_hist)\n",
    "        plt.show()\n",
    "  \n",
    "    def loss_function(self,y_,y): \n",
    "        m = len(y)\n",
    "        if self.loss_method == 'logloss':\n",
    "            y_ /= y_.sum(axis=1)[:, np.newaxis]\n",
    "            return -1 * np.sum(y * np.log(y_))/len(y)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "    def hid_activation(self,x): \n",
    "        if self.activation_method == 'sigmoid':\n",
    "            return 1./(1+np.exp(-x))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def grad_activation(self,x):\n",
    "        if self.activation_method == 'sigmoid': \n",
    "            return x * (1-x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def out_activation(self, x): \n",
    "        if self.loss_method == 'logloss':\n",
    "            return np.exp(x)/np.sum(np.exp(x),axis=1)[:, np.newaxis] \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z1 = np.dot(x,self.w1)+self.b1\n",
    "        h1 = self.hid_activation(z1)\n",
    "        z2 = np.dot(h1,self.w2)+self.b2\n",
    "        h2 = self.hid_activation(z2)\n",
    "        z3 = np.dot(h2,self.w3)+self.b3\n",
    "        y_ = self.out_activation(z3) \n",
    "        return(h1,h2,y_)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        (h1, h2, y) = self.forward(x)\n",
    "        y_ = np.zeros(y.shape[0],int) \n",
    "        # array to reshape predictions to (178,)\n",
    "        for i in range(len(y)):\n",
    "            for j in range(len(y[i])):\n",
    "                y[i,j] = round(y[i,j],0)\n",
    "            y_[i] = np.argmax(y[i])\n",
    "        return y_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain the data\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "data=load_wine()\n",
    "x, Y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess the data\n",
    "\n",
    "As usual, it is important to process the data as follows:\n",
    "- x, it is recommended to normalize x for NN before the fitting through the `StandardScaler` in sklearn. \n",
    "- Y, if it is a classification problem, we need to transform Y to a set of arrays with 0 or 1. For instance, there exist four classes (1, 2, 3, 4), we need to transform them to [1,0,0,0], [0,1,0,0], [0,0,1,0] and [0,0,0,1]. Fortunately, this can be done via `OneHotEncoder` in sklearn as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      "[ 1.51861254 -0.5622498   0.23205254 -1.16959318  1.91390522  0.80899739\n",
      "  1.03481896 -0.65956311  1.22488398  0.25171685  0.36217728  1.84791957\n",
      "  1.01300893]\n",
      "0\n",
      "[1. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()  \n",
    "scaler.fit(x)  \n",
    "x0 = scaler.transform(x)\n",
    "Y0 = OneHotEncoder().fit_transform(np.expand_dims(Y, axis=1)).toarray()\n",
    "print(x[0])\n",
    "print(x0[0])\n",
    "print(Y[0])\n",
    "print(Y0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation function: sigmoid, loss function: logloss =    1.447, iterations: 0, learning rate: 0.0100\n",
      "activation function: sigmoid, loss function: logloss =    0.045, iterations: 100, learning rate: 0.0090\n",
      "activation function: sigmoid, loss function: logloss =    0.019, iterations: 199, learning rate: 0.0082\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3XmYXGWd9vHvXdVLtk5n69DZIAkESAJhmZZhRBEvkUBgQB1QcBmdwWGcwRl9XUZ4dZSXcRa38XVhVHQQYRQUkREVDbIIIxqkWSUJgRCWhCSks++91W/+qNOdStPd6cQ+VdVd9+e6+qpTz3nq1K9PV/fd5zmbIgIzMzOATKkLMDOz8uFQMDOzbg4FMzPr5lAwM7NuDgUzM+vmUDAzs24OBasYkr4u6R8Hu6/ZcCKfp2BDgaTngfdGxF2lrsVsOPOWgg0LkqpKXUMxVdr3a8XjULCyJ+lG4HDgJ5J2SvoHSTMlhaRLJb0I3JP0vUXSeknbJN0vaX7Bcq6X9Olk+gxJayR9WNIGSesk/cUh9p0o6SeStkt6SNKnJf26n+/nNZJ+I2mrpNWS3pO0/0rSewv6vadwOcn3e7mkZ4BnkiGuz/dY9o8lfSiZnirpVkktkp6T9PeH9AOwiuJQsLIXEe8CXgT+NCLGRMRnC2a/DpgLLEye/xyYA0wGHgG+28+iG4F6YBpwKXCNpPGH0PcaYFfS593JV68kHZ7U+BWgATgReKyfGnt6E/DHwDzge8DbJClZ9njgLOBmSRngJ8DjSc1vAD4oaWGvSzVLOBRsqLsqInZFxB6AiLguInZERCtwFXCCpPo+XtsOXB0R7RFxB7ATOOZg+krKAn8GfCoidkfEMuA7/dT7DuCuiLgpWdamiDiYUPjXiNicfL//AwTw2mTehcBvI2It8CqgISKujoi2iFgFfBO4+CDeyyqQxyVtqFvdNZH8gf5n4CLy/4XnklmTgG29vHZTRHQUPN8NjOnjffrq20D+92h1wbzC6Z5mAM/2M/9AupcdESHpZuAS4H7g7cB/JbOPAKZK2lrw2iz5IDHrk7cUbKjo6zC5wva3AxcAZ5If6pmZtCu9smgBOoDpBW0z+um/Gjiyj3m7gFEFzxt76dNzPdwEXCjpCPLDSrcWvM9zETGu4KsuIhb1U5uZQ8GGjJeB2QfoUwe0ApvI/3H9l7SLiohO4EfAVZJGSToW+PN+XvJd4ExJb5VUleykPjGZ9xjwlmQ5R5Hfd3Gg93+UfDB9C1gcEV1bBr8Dtkv6mKSRkrKSjpP0qkP8Vq1COBRsqPhX4BPJETsf6aPPDcALwEvAMmBJkWp7P/ktk/XAjeT/e2/trWNEvAgsAj4MbCYfBCcks78ItJEPwO/Q/07yQjeR3zr6XsH7dAJ/Sn5H9nPARvLB0df+FTPAJ6+ZDTpJnwEaI6LPo5DMypW3FMz+QJKOlbRAeaeQH/a5rdR1mR0KH31k9oerIz+EMxXYAHwB+HFJKzI7RB4+MjOzbh4+MjOzbkNu+GjSpEkxc+bMUpdhZjakPPzwwxsjouFA/YZcKMycOZPm5uZSl2FmNqRIemEg/Tx8ZGZm3RwKZmbWzaFgZmbdHApmZtbNoWBmZt0cCmZm1s2hYGZm3SomFFas38EX7lzBpp29XtHYzMyooFB4tmUnX7lnJS0OBTOzPlVMKNRW5b/Vto7cAXqamVWuCgqFLACtDgUzsz5VTCjUJFsKre0OBTOzvqQWCpKuk7RB0pMH6PcqSZ2SLkyrFtg3fNTa0Znm25iZDWlpbilcD5zdXwdJWeAzwOIU6wCgttr7FMzMDiS1UIiI+4HNB+j2d8Ct5G9hmCrvUzAzO7CS7VOQNA14M/D1AfS9TFKzpOaWlpZDej8PH5mZHVgpdzT/f+BjEXHAv9IRcW1ENEVEU0PDAW8c1KvuHc3eUjAz61Mp77zWBNwsCWASsEhSR0T8dxpvVuujj8zMDqhkoRARs7qmJV0P/DStQIB9+xTaOh0KZmZ9SS0UJN0EnAFMkrQG+BRQDRARB9yPMNiqs0KC1nbvUzAz60tqoRARlxxE3/ekVUcXSdRWZbxPwcysHxVzRjNATdahYGbWn4oKhdrqrA9JNTPrR2WFgoePzMz65VAwM7NuFRUKNVVZn6dgZtaPigqF/JaC9ymYmfWl4kLBV0k1M+tbZYVCddb7FMzM+lFZoeAdzWZm/aqoUKjxPgUzs35VVCjUVmV89JGZWT8qLBSyvkqqmVk/KiwUMr5KqplZPyorFKq9o9nMrD+VFQrJVVIjotSlmJmVpcoKher83dfaOx0KZma9qaxQ6LpPsw9LNTPrVYWGgvcrmJn1JrVQkHSdpA2Snuxj/jskPZF8/UbSCWnV0qW2Kj985FAwM+tdmlsK1wNn9zP/OeB1EbEA+Cfg2hRrAfJnNAM+LNXMrA9VaS04Iu6XNLOf+b8peLoEmJ5WLV26ho98ApuZWe/KZZ/CpcDP+5op6TJJzZKaW1paDvlNaqu7thQcCmZmvSl5KEh6PflQ+FhffSLi2ohoioimhoaGQ34v71MwM+tfasNHAyFpAfAt4JyI2JT2+9X4kFQzs36VbEtB0uHAj4B3RcTTxXjP7n0K3lIwM+tValsKkm4CzgAmSVoDfAqoBoiIrwOfBCYC/yEJoCMimtKqBzx8ZGZ2IGkefXTJAea/F3hvWu/fG5/RbGbWv5LvaC4mH31kZta/igqFmqwvc2Fm1p+KCoWuq6R6R7OZWe8qKxS8T8HMrF8VFQpVGZGRh4/MzPpSUaEgidqqrEPBzKwPFRUKkD+r2fsUzMx6V3GhUFuV8T4FM7M+VF4oVGd8noKZWR8qLxS8T8HMrE8VFwojq7PsbO0odRlmZmWp4kJh1qTRrNyws9RlmJmVpYoLhWOn1PHS1j1s291e6lLMzMpOxYXC3CljAVi+fnuJKzEzKz8VFwrzukJhnUPBzKyniguFyXW1TBhd41AwM+tFxYWCJOZOqWP5uh2lLsXMrOxUXCgAzG0cy4qXd9DR6fMVzMwKVWYoTBlLW0eOVRt3lboUM7OyklooSLpO0gZJT/YxX5K+LGmlpCcknZxWLT2dMmsCAPc/3VKstzQzGxLS3FK4Hji7n/nnAHOSr8uAr6VYy35mTBjFsY113Lns5WK9pZnZkJBaKETE/cDmfrpcANwQeUuAcZKmpFVPT2fNb6T5+c1s2tlarLc0Myt7pdynMA1YXfB8TdL2CpIuk9QsqbmlZXCGfM6adxi5gLuf2jAoyzMzGw5KGQrqpS166xgR10ZEU0Q0NTQ0DMqbz586lmnjRnLn0vWDsjwzs+GglKGwBphR8Hw6sLZYby6JhfMbuf/pjWzf6+sgmZlBaUPhduDPk6OQTgW2RcS6YhZw3glTaOvMcZd3OJuZAekeknoT8FvgGElrJF0q6X2S3pd0uQNYBawEvgn8bVq19OWkGeOYNm4kP32iqFlkZla2qtJacERccoD5AVye1vsPhCTOXTCFbz/wHNt2t1M/qrqU5ZiZlVxFntFc6Nzjp9DeGSxe5h3OZmYVHwoLptczY8JIfuYhJDMzh4Ikzj1+Kg+s3MiWXW2lLsfMrKQqPhQAzlswhY5csNjnLJhZhXMokD+RbebEUfzs9x5CMrPK5lBg31FIv3l2k6+FZGYVzaGQOPf4qXTmgl94CMnMKphDITF3Sh2zG0b7KCQzq2gOhYQkzjt+CktWbaJlh4eQzKwyORQKnLtgKrmAXzzprQUzq0wOhQLHNNYxZ/IYfuIhJDOrUA6FHs5dMIWHnt/My9v3lroUM7Oicyj0cN6CKUTAz33OgplVIIdCD0dNruPYxjpfTtvMKpJDoRfnHj+F5he2sG7bnlKXYmZWVA6FXpy7YAqAz1kws4rjUOjF7IYxzJsy1tdCMrOK41Dow3knTOHRF7eyZsvuUpdiZlY0DoU+nHt8fgjpDm8tmFkFSTUUJJ0taYWklZKu6GX+4ZLulfSopCckLUqznoNxxMTRHD+t3vsVzKyipBYKkrLANcA5wDzgEknzenT7BPCDiDgJuBj4j7TqORTnLZjC42u28eImDyGZWWVIc0vhFGBlRKyKiDbgZuCCHn0CGJtM1wNrU6znoC1KhpC8w9nMKkWaoTANWF3wfE3SVugq4J2S1gB3AH/X24IkXSapWVJzS0tLGrX2asaEUZw4Yxw/faKsssrMLDVphoJ6aYsezy8Bro+I6cAi4EZJr6gpIq6NiKaIaGpoaEih1L4tOr6RpWu3s3qzh5DMbPhLMxTWADMKnk/nlcNDlwI/AIiI3wIjgEkp1nTQFs5vBGCx78hmZhVgQKEg6QOSxirvPyU9IumsA7zsIWCOpFmSasjvSL69R58XgTck7zGXfCgUb3xoAI6YOJpjG+u4c+nLpS7FzCx1A91S+MuI2A6cBTQAfwH8W38viIgO4P3AYmA5+aOMlkq6WtL5SbcPA38l6XHgJuA9EdFziKnkFs5v5KEXNvuObGY27A00FLr2DywCvh0Rj9P7PoP9RMQdEXF0RBwZEf+ctH0yIm5PppdFxGkRcUJEnBgRdx7KN5G2hfMbiYC7lntrwcyGt4GGwsOS7iQfCosl1QG59MoqL3On1DFjwkjvVzCzYW+goXApcAXwqojYDVSTH0KqCJJYOK+R36zcxI697aUux8wsNQMNhT8BVkTEVknvJH8m8rb0yio/C49rpK0zx70rymo/uJnZoBpoKHwN2C3pBOAfgBeAG1KrqgydfPh4Jo2p9RCSmQ1rAw2FjuSooAuAL0XEl4C69MoqP9mMeOO8w/jVUxvY295Z6nLMzFIx0FDYIelK4F3Az5KL3VWnV1Z5Wjj/MHa1dfLAyo2lLsXMLBUDDYW3Aa3kz1dYT/4aRp9Lraoy9eojJ1FXW+UhJDMbtgYUCkkQfBeol3QesDciKmqfAkBNVYbXHzuZu5ZvoKOzYo7INbMKMtDLXLwV+B1wEfBW4EFJF6ZZWLlaOL+RzbvaaH5hS6lLMTMbdFUD7Pdx8ucobACQ1ADcBfwwrcLK1RnHNFBTlWHx0vWcOntiqcsxMxtUA92nkOkKhMSmg3jtsDK6torT50zizqUvU4aXaTIz+4MM9A/7LyQtlvQeSe8Bfkb+pjgV6az5jby0dQ9L124vdSlmZoNqQMNHEfFRSX8GnEb+QnjXRsRtqVZWxs6cexgZwS+eXM9x0+pLXY6Z2aAZ6D4FIuJW4NYUaxkyJoyu4ZRZE1i8dD0fWXhMqcsxMxs0/Q4fSdohaXsvXzskVfTYycL5jTyzYSerWnaWuhQzs0HTbyhERF1EjO3lqy4ixharyHK07zadvseCmQ0fFXkE0WCYOm4kC6bX++xmMxtWHAp/gIXzG3ls9VbWb9tb6lLMzAZFqqEg6WxJKyStlHRFH33eKmmZpKWSvpdmPYNt4fzDAPjlMm8tmNnwkFooJFdSvQY4B5gHXCJpXo8+c4ArgdMiYj7wwbTqScNRk+uY3TCaX3gIycyGiTS3FE4BVkbEqohoA24mfz+GQn8FXBMRWwB6nDU9JCyc38iSVZvZurut1KWYmf3B0gyFacDqgudrkrZCRwNHS3pA0hJJZ6dYTyoWzm+kMxfcvXzI5ZmZ2SukGQrqpa3nxYKqgDnAGcAlwLckjXvFgqTLJDVLam5pKa97JC+YVs+U+hE+CsnMhoU0Q2ENMKPg+XRgbS99fhwR7RHxHLCCfEjsJyKujYimiGhqaGhIreBDkcmIs+Ydxv3PtLCnzbfpNLOhLc1QeAiYI2mWpBrgYuD2Hn3+G3g9gKRJ5IeTVqVYUyoWzm9kb3uO+54ur60YM7ODlVooREQH8H5gMbAc+EFELJV0taTzk26LgU2SlgH3Ah+NiE1p1ZSWU2ZNYNyoau70EJKZDXEDviDeoYiIO+hxie2I+GTBdAAfSr6GrKpshjccexi/XLae9s4c1VmfE2hmQ5P/eg2ShfMPY/veDpasGnIbOmZm3RwKg+T0oxsYWZ3l5096CMnMhi6HwiAZUZ3ljfMO447fr6O1w0chmdnQ5FAYRG85eRpbd7dz71M+CsnMhiaHwiB6zVGTmDSmltseXVPqUszMDolDYRBVZTO86cSp3PPUBrbs8rWQzGzocSgMsrecPJ32zuCnT/Q8edvMrPw5FAbZvKljObaxjlsfeanUpZiZHTSHQgrecvI0Hlu9lVUtO0tdipnZQXEopOCCE6eREfzIWwtmNsQ4FFJw2NgRnH50A7c8vJr2zlypyzEzGzCHQkre+cdH8PL2Vu5a9nKpSzEzGzCHQkpef+xkpo0byQ2/faHUpZiZDZhDISXZjHjHqYfz21WbWLlhR6nLMTMbEIdCit7WNIOabIYbvbVgZkOEQyFFE8fUcu6CKdz6yEvsau0odTlmZgfkUEjZu/7kCHa2dnDboz481czKn0MhZSfNGMdx08by7QeeI5eLUpdjZtYvh0LKJHHZ6UfybMsuFvsezmZW5lINBUlnS1ohaaWkK/rpd6GkkNSUZj2lcu7xU5g1aTRfvXcl+dtSm5mVp9RCQVIWuAY4B5gHXCJpXi/96oC/Bx5Mq5ZSy2bE35xxJEvXbudXT/sGPGZWvtLcUjgFWBkRqyKiDbgZuKCXfv8EfBbYm2ItJffmk6YxbdxIrrnHWwtmVr7SDIVpwOqC52uStm6STgJmRMRP+1uQpMskNUtqbmkZmv9pV2cz/PXrZtP8whYefG5zqcsxM+tVmqGgXtq6/0WWlAG+CHz4QAuKiGsjoikimhoaGgaxxOJ6a9MMJo2p5ct3P+OtBTMrS2mGwhpgRsHz6UDh7cjqgOOAX0l6HjgVuH247mwGGFGd5fLXH8lvnt3Er1YMzS0eMxve0gyFh4A5kmZJqgEuBm7vmhkR2yJiUkTMjIiZwBLg/IhoTrGmknvnqUcwe9JoPv2zZb6stpmVndRCISI6gPcDi4HlwA8iYqmkqyWdn9b7lrvqbIYrF83l2ZZd3PS7F0tdjpnZfqrSXHhE3AHc0aPtk330PSPNWsrJmXMn8+ojJ/LFXz7NBSdOo35kdalLMjMDfEZzSUji4+fOZeuedr56zzOlLsfMrJtDoUTmT63nrX80g28/8DxL124rdTlmZoBDoaSuXHQs40fX8NFbnvBOZzMrCw6FEho3qoZPv+k4lq3bztd+9WypyzEzcyiU2sL5jZx/wlS+cs8zPLV+e6nLMbMK51AoA1edP5/6kdV8+AeP09rRWepyzKyCORTKwITRNfzLm49n6drtfPqny0tdjplVMIdCmThrfiOXnT6bG5e8wI8f8607zaw0HApl5KMLj+FVM8dzxa2/55mXd5S6HDOrQA6FMlKdzfDVt5/M6Noq/vq/Hmbb7vZSl2RmFcahUGYOGzuCa95+Eqs37+ayG5u949nMisqhUIb+ePZEPn/RCTz43GY+cssT5HK+94KZFUeqF8SzQ3fBidNYu3Uvn/nFU0ypH8H/XTS31CWZWQVwKJSx971uNmu37uHa+1cxuqaKD5w5p9Qlmdkw51AoY5K46vz57G7r5It3PU1G8HdvcDCYWXocCmUumxGfvXABEcEXfvk0mYy4/PVHlbosMxumHApDQDYjPnfRCQTwucUr2LG3g4+dfQySSl2amQ0zDoUhIpsRn7/oBEbVZPn6fc+yaWcr//qW46nK+gAyMxs8DoUhJJsRn37TcUwaU8uX7n6GLbvb+PIlJzGqxj9GMxscqf6bKelsSSskrZR0RS/zPyRpmaQnJN0t6Yg06xkOJPF/3ng0//Sm47jnqQ1c+LXf8tLWPaUuy8yGidRCQVIWuAY4B5gHXCJpXo9ujwJNEbEA+CHw2bTqGW7edeoRXPeeV7F6827O/8qvaX5+c6lLMrNhIM0thVOAlRGxKiLagJuBCwo7RMS9EbE7eboEmJ5iPcPOGcdM5rbLT6NuRBWXfHMJ337gOSJ89rOZHbo0Q2EasLrg+ZqkrS+XAj/vbYakyyQ1S2puaWkZxBKHvqMmj+HHl7+G0+c08P9+sozLbnyYrbvbSl2WmQ1RaYZCb8dL9vpvrKR3Ak3A53qbHxHXRkRTRDQ1NDQMYonDQ/2oar717iY+ce5cfrViA4u+9D88sHJjqcsysyEozVBYA8woeD4dWNuzk6QzgY8D50dEa4r1DGuSeO9rZ3Pr37ya2uos7/jWg1z5oyfYvteX3zazgUszFB4C5kiaJakGuBi4vbCDpJOAb5APhA0p1lIxFkwfx88/8Fr++vTZfP+h1Zz17/fzsyfWeV+DmQ1IaqEQER3A+4HFwHLgBxGxVNLVks5Pun0OGAPcIukxSbf3sTg7CCOqs1y5aC63/e1pjB9dw+Xfe4SLr13CsrXbS12amZU5DbX/IJuamqK5ubnUZQwZnbng5ode5POLV7BtTztvPmk6HzxzDjMmjCp1aWZWRJIejoimA/ZzKFSGbbvb+co9z3DDkhfI5YK3vWoGf3PGkUwf73AwqwQOBevV+m17uebeldz80IvkAs45rpG/eu1sTpgxrtSlmVmKHArWr7Vb93D9b57npgdfZEdrB6fMnMBfvmYWb5g7mWpfZM9s2HEo2IDs2NvO9x9azbcfeJ6Xtu5h0pha3nzSVC5qmsHRh9WVujwzGyQOBTsoHZ057l3Rwi3Nq7nnqQ105IITptdzwYnTOGv+Yd73YDbEORTskG3a2cp/P7aWHz68huXr8oexHjdtLAvnNXLW/EaOPmyMb/BjNsQ4FGxQPLdxF3cuXc8vlq7n0Re3AtA4dgSnHTWJ18yZyGlHTmLy2BElrtLMDsShYIPu5e17ueepDfz6mY088OxGtu7OX0LjyIbRnHz4eE4+YjwnHT6OOZPryGa8JWFWThwKlqpcLli2bjv/88xGmp/fzCMvbmFLEhJjaqs4YUY9J84Yx9wpYzm2cSyzJo12UJiV0EBDwfdxtEOSyYjjptVz3LR64Egighc27ebR1Vt45IWtPLp6C9+4bxUdufw/HbVVGY5prOPYxjqOaRzL7IbRzJ40mmnjRvo+02ZlxFsKlprWjk5WbtjJU+t28NT67Sxft4Pl67azade++z1UZ8URE0cza1I+JGZNGs3hE0YxbfxIGutHUFuVLeF3YDZ8eEvBSq62Ksv8qfXMn1q/X/umna08t3EXqzbu4rmNu3iuJf9439MttHXkuvtJ0DCmlqnjRjJt/Eimjdv31Vg/gslja5k4utbDUmaDyKFgRTdxTC0Tx9TSNHPCfu25XPDS1j2s3rKbtVv38tKWPby0NT+9bO12frns5f1CAyCj/PIaxtQyeWz+saGulsl1tTTU5YNjwugaxo+qoX5ktQPE7AAcClY2MhkxY8KoPq/gmssFm3a18dLWPazftoeWHa207GhlQ/LYsrOVp9btYOPO1u59GYUkqB9ZzfhRNYwbVc2EUTWMG1XD+FHVjE+CY/yoaupHVTN2RDV1I6qoSx596Q+rFA4FGzIyGdFQl98SoJ8L+OVywdY97WzYsZcN21vZsruNLbva2Ly7na2729iyu50tu9pYv30vT63fweZdbexp7+z3vUdWZ5OQ2BcUY0dWM7bree2+eaNrs4yqqWJUTf5xdG2WkTVZRtdUMbI6S8ZbK1bGHAo27GQyYsLoGiaMruHYxoG9Zm97J1t3t7N5Vxvb9rSzY2872/d2sGNvOzv2e+xgezLvpa17uuftbc8d+E0SI6uzPYIjy+jaqqR9X9vI6iy11VlqqzKMqM4mXxlGVBVMJ4+1VVlqu55XZanOymed2yFxKJiRv1tdY32WxvpDOzu7rSPHztZ8QOxu62R3Wwe7Wjv3Tbd1sqe7rSNp72RXawd72vOPLTta93vt3o5ODvXgwIzYFyRVme5wqa3OUpvNUFOVoToraqoy1CQhUluVoSaboTqZn++Tybcn0zXZPtqruuaJmmyW6ipRlclQlRFVWVGdzU9nMw6rcudQMBsENVUZJlTlt04GS0TQ1pljb3uO1vZO9rbn2NvRyd6u6fZOWjtyyfNO9nZ09ds3P98/193W2tFJe2eO3W0dtHcGbR052jpz+z22J4+97ZcZDNXZJDCySkIjQ3XyWJUV1T3mVWWSUEleV53Nh0tX0FRlM93L7JpXlc2QVT6QMhLZDGQzGbKCbDKvuy0DGeVf3z2dvK4qkyGTocey9n1VZV7Zli14ninoM1RCMdVQkHQ28CUgC3wrIv6tx/xa4Abgj4BNwNsi4vk0azIbKiTlh4WqsjCyuujvn8vlQ6lnWOwfIF3B0klbR+zXt6MzP78zF7TncnR0Rr4tF/vN68jlpwvndXQGHQXzdrd10JGL5DX5+V3LbO/M9+vY77F8z7+S2C9MMhIZsW+6IFiUtGeVn77klMN572tnp1pfaqEgKQtcA7wRWAM8JOn2iFhW0O1SYEtEHCXpYuAzwNvSqsnMBi6TESMy+SGooSYiyEX+HuWduaAzgs7O5LGgLZfLB0hnLshF0NGZPPbS1v26XI/l9NLW83UduX3vlevRrzNHd1suomA6H8ydsW960pja1NddmlsKpwArI2IVgKSbgQuAwlC4ALgqmf4h8FVJiqF2mrWZlRVJ+aEiH+l10NI8+HoasLrg+Zqkrdc+EdEBbAMmpliTmZn1I81Q6C2ie24BDKQPki6T1CypuaWlZVCKMzOzV0ozFNYAMwqeTwfW9tVHUhVQD2zuuaCIuDYimiKiqaGhIaVyzcwszVB4CJgjaZakGuBi4PYefW4H3p1MXwjc4/0JZmalk9qO5ojokPR+YDH5Q1Kvi4ilkq4GmiPiduA/gRslrSS/hXBxWvWYmdmBpXqeQkTcAdzRo+2TBdN7gYvSrMHMzAbOl340M7NuDgUzM+s25G7HKakFeOEQXz4J2DiI5Qymcq3NdR2ccq0Lyrc213VwDrWuIyLigIdvDrlQ+ENIah7IPUpLoVxrc10Hp1zrgvKtzXUdnLTr8vCRmZl1cyiYmVm3SguFa0tdQD/KtTbXdXDKtS4o39pc18FJta6K2qdgZmb9q7QtBTMz64dDwczMulVMKEg6W9IKSSslXVHCOmZIulfScklLJX0gab9K0kuSHku+FpWgtucl/T55/+akbYKkX0p6JnkcX4K6jilYL49J2i7pg6VYZ5Kuk7RB0pMFbb2uI+WMaEvGAAAFg0lEQVR9OfnMPSHp5CLX9TlJTyXvfZukcUn7TEl7Ctbb14tcV58/N0lXJutrhaSFadXVT23fL6jreUmPJe3FXGd9/Y0ozucsIob9F/kL8j0LzAZqgMeBeSWqZQpwcjJdBzwNzCN/B7qPlHg9PQ9M6tH2WeCKZPoK4DNl8LNcDxxRinUGnA6cDDx5oHUELAJ+Tv6+IacCDxa5rrOAqmT6MwV1zSzsV4L11evPLfk9eByoBWYlv7PZYtbWY/4XgE+WYJ319TeiKJ+zStlS6L41aES0AV23Bi26iFgXEY8k0zuA5bzyjnTl5ALgO8n0d4A3lbAWgDcAz0bEoZ7V/geJiPt55T0/+lpHFwA3RN4SYJykKcWqKyLujPwdDQGWkL+nSVH1sb76cgFwc0S0RsRzwEryv7tFr02SgLcCN6X1/n3p529EUT5nlRIKA7k1aNFJmgmcBDyYNL0/2fy7rhTDNOTvenenpIclXZa0HRYR6yD/YQUml6CuQhez/y9qqdcZ9L2Oyulz95fk/5vsMkvSo5Luk/TaEtTT28+tnNbXa4GXI+KZgrair7MefyOK8jmrlFAY0G0/i0nSGOBW4IMRsR34GnAkcCKwjvyma7GdFhEnA+cAl0s6vQQ19En5mzWdD9ySNJXDOutPWXzuJH0c6AC+mzStAw6PiJOADwHfkzS2iCX19XMri/WVuIT9//ko+jrr5W9En117aTvk9VYpoTCQW4MWjaRq8j/s70bEjwAi4uWI6IyIHPBNUtxs7ktErE0eNwC3JTW83LUpmjxuKHZdBc4BHomIl6E81lmir3VU8s+dpHcD5wHviGQAOhme2ZRMP0x+7P7oYtXUz8+t5OsLum8N/Bbg+11txV5nvf2NoEifs0oJhYHcGrQokrHK/wSWR8S/F7QXjgG+GXiy52tTrmu0pLquafI7KZ9k/1umvhv4cTHr6mG//95Kvc4K9LWObgf+PDk65FRgW9fmfzFIOhv4GHB+ROwuaG+QlE2mZwNzgFVFrKuvn9vtwMWSaiXNSur6XbHqKnAm8FRErOlqKOY66+tvBMX6nBVjb3o5fJHfQ/80+YT/eAnreA35TbsngMeSr0XAjcDvk/bbgSlFrms2+SM/HgeWdq0jYCJwN/BM8jihROttFLAJqC9oK/o6Ix9K64B28v+hXdrXOiK/WX9N8pn7PdBU5LpWkh9r7vqcfT3p+2fJz/hx4BHgT4tcV58/N+DjyfpaAZxT7J9l0n498L4efYu5zvr6G1GUz5kvc2FmZt0qZfjIzMwGwKFgZmbdHApmZtbNoWBmZt0cCmZm1s2hYFZEks6Q9NNS12HWF4eCmZl1cyiY9ULSOyX9Lrl2/jckZSXtlPQFSY9IultSQ9L3RElLtO++BV3XuT9K0l2SHk9ec2Sy+DGSfqj8vQ6+m5zBalYWHApmPUiaC7yN/AUCTwQ6gXcAo8lfe+lk4D7gU8lLbgA+FhELyJ9R2tX+XeCaiDgBeDX5s2chf9XLD5K/Rv5s4LTUvymzAaoqdQFmZegNwB8BDyX/xI8kf/GxHPsukvZfwI8k1QPjIuK+pP07wC3JdaSmRcRtABGxFyBZ3u8iua6O8nf2mgn8Ov1vy+zAHApmryTgOxFx5X6N0j/26NffNWL6GxJqLZjuxL+HVkY8fGT2SncDF0qaDN33xj2C/O/LhUmftwO/johtwJaCm668C7gv8te/XyPpTckyaiWNKup3YXYI/B+KWQ8RsUzSJ8jfhS5D/iqalwO7gPmSHga2kd/vAPnLGH89+aO/CviLpP1dwDckXZ0s46Iifhtmh8RXSTUbIEk7I2JMqeswS5OHj8zMrJu3FMzMrJu3FMzMrJtDwczMujkUzMysm0PBzMy6ORTMzKzb/wJBBADco85kjQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### mlp parameters\n",
    "my_mlp = my_MLPClassifier(hidden=[4,2], max_iter=200, learning_rate=0.01)\n",
    "my_mlp.fit(x0, Y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "y_ = my_mlp.predict(x0)\n",
    "count = len(y_[y_==Y])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after  200 iterations is 100.00%\n"
     ]
    }
   ],
   "source": [
    "print('accuracy after {:4d} iterations is {:6.2f}%'.format(my_mlp.iterations, 100*count/len(y_)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
