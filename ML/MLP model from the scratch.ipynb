{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron from the scratch\n",
    "\n",
    "Nowadays, multi-Layer neural network has been proved to be a powerful tool in many data science problems. Though many existing packages have provided the interfaces to call this function (e.g. scikit-learn), it would be good to write some toy model by your own. Through this practice, you will gain some experience in software engineering. More importantly, you will understand the underlying mathmatics better and know how to fix the troubles when you run the code from the existing softwares. In the tutorial, we will continue to use the wine data and figure out how to write our own MLP classfier.\n",
    "\n",
    "Let us start with the example in the previous lecture\n",
    "```\n",
    "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(4, 2), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
    "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
    "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
    "```\n",
    "\n",
    "You should be able to understand most of the parameters at the moment. To realize a minimum version of MLP, we can try to implement the following parameters into our model:\n",
    "- hidden_layer_sizes: to make life easier, let us just consider 2 hidden layer models\n",
    "- max_iter: maximum number of iteractions\n",
    "- learning_rate_init: \n",
    "\n",
    "Note that we will completely ignore the terms related to regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_MLPClassifier(object):\n",
    "    \"\"\"\n",
    "    Basic MultiLayer Perceptron (MLP) neural network.\n",
    "    Args:\n",
    "    hidden layer: []\n",
    "    max_iterations: []\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden, max_iterations = 50, learning_rate = 0.01, \n",
    "                decay_rate = 0.99):\n",
    "        \"\"\"\n",
    "        :param hidden: number of hidden neurons\n",
    "        :param iterations: how many epochs\n",
    "        :param learning_rate: initial learning rate\n",
    "        \"\"\"\n",
    "        # initialize parameters\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.rate_decay = rate_decay\n",
    "\n",
    "        # create randomized weights\n",
    "        self.w1 = np.random.randn(dim_in, n_hid1)\n",
    "        self.w2 = np.random.randn(n_hid1, n_hid2)\n",
    "        self.w3 = np.random.randn(n_hid2, dim_out)\n",
    "        self.b1 = np.random.randn(1,n_hid1)\n",
    "        self.b2 = np.random.randn(1,n_hid2)\n",
    "        self.b3 = np.random.randn(1,dim_out)\n",
    "        \n",
    "    def fit(self, input, target):      \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "                \n",
    "        for i in range(self.iterations):\n",
    "            # forward function\n",
    "            self.forward(inputs)\n",
    "                \n",
    "            # evaluate and print the loss function    \n",
    "            error += self.backPropagate()\n",
    "                \n",
    "            # backpropagation\n",
    "            self.backpropagate()\n",
    "            \n",
    "            # updating the weight\n",
    "            learning_rate = self.learning_rate * (self.decay_rate**i)\n",
    "            #self.w1 -= \n",
    "            #self.w2 -=\n",
    "            #self.w3 -=\n",
    "            #self.b1 -= \n",
    "            #self.b2 -=\n",
    "            #self.b3 -=\n",
    "            \n",
    "    def forward(self, inputs):    \n",
    "        pass\n",
    "\n",
    "    def backPropagate(self, targets):\n",
    "        pass\n",
    "    \n",
    "    def loss():\n",
    "        pass\n",
    "    \n",
    "    def grad_loss():\n",
    "        pass\n",
    "\n",
    "    def activation():\n",
    "        pass\n",
    "    \n",
    "    def grad_activation():\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        return list of predictions after training algorithm\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for p in X:\n",
    "            predictions.append(self.forward(p))\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
