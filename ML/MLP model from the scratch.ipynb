{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron from the scratch\n",
    "\n",
    "Nowadays, multi-Layer neural network has been proved to be a powerful tool in many data science problems. Though many existing packages have provided the interfaces to call this function (e.g. scikit-learn), it would be good to write some toy model by your own. Through this practice, you will gain some experience in software engineering. More importantly, you will understand the underlying mathmatics better and know how to fix the troubles when you run the code from the existing softwares. In the tutorial, we will continue to use the wine data and figure out how to write our own MLP classfier.\n",
    "\n",
    "Let us start with the example in the previous lecture\n",
    "```\n",
    "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(4, 2), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
    "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
    "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
    "```\n",
    "<img src=\"img/MLP.jpeg\" style=\"width: 800px;\"/>\n",
    "<center> Figure 1, the MLP model used in this lecture</center>\n",
    "\n",
    "You should be able to understand most of the parameters at the moment. To realize a minimum version of MLP, we can try to implement the following parameters into our model:\n",
    "- hidden_layer_sizes: to make life easier, let us just consider 2 hidden layer models\n",
    "- max_iter: maximum number of iteractions\n",
    "- learning_rate_init: \n",
    "\n",
    "Note that we will completely ignore the terms related to regularization\n",
    "\n",
    "## Back propagation\n",
    "$$\\frac{\\partial L}{\\partial y} = y-Y$$\n",
    "$$\\frac{\\partial y}{\\partial f_3} = $$\n",
    "$$\\frac{\\partial f_3}{\\partial h_2} = $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class my_MLPClassifier(object):\n",
    "    \"\"\"\n",
    "    Basic MultiLayer Perceptron (MLP) neural network.\n",
    "    Args:\n",
    "    hidden layer: []\n",
    "    max_iterations: []\n",
    "    \"\"\"\n",
    "    def __init__(self, hiddenlayers=[4,4], activation = 'sigmoid', max_iterations = 50, learning_rate = 0.01, \n",
    "                decay_rate = 0.99, loss_method='mse'):\n",
    "        \"\"\"\n",
    "        :param hidden: number of hidden neurons\n",
    "        :param iterations: how many epochs\n",
    "        :param learning_rate: initial learning rate\n",
    "        \"\"\"\n",
    "        self.hiddenlayers = hiddenlayers\n",
    "        \n",
    "        # initialize input parameters\n",
    "        self.iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.n_hid1, self.n_hid2 = self.hiddenlayers[0], self.hiddenlayers[1]\n",
    "        self.loss_method= loss_method\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"The neural network scheme is performed.\"\"\"\n",
    "        \n",
    "        if type(X) is np.ndarray:\n",
    "            self.X = X\n",
    "        else:\n",
    "            self.X = np.asarray(X)\n",
    "            \n",
    "        self.n_features = self.X.shape[1]\n",
    "        \n",
    "        # Initializing random weights\n",
    "        self.weights = self.init_random_weights(self.hiddenlayers, self.n_features)\n",
    "        \n",
    "        # Forward\n",
    "        for i in range(len(self.X)):\n",
    "            X = np.hstack(([1.], self.X[i]))\n",
    "            self.output = self.forward(X, self.weights, self.hiddenlayers)\n",
    "        \n",
    "        \n",
    "        #for i in range(self.iterations):\n",
    "        #    # forward function\n",
    "        #    (h1, h2, y) = self.forward(input)\n",
    "            \n",
    "            # evaluate and print the loss function    \n",
    "        #    loss = self.loss(y, target)\n",
    "                \n",
    "            # backpropagation\n",
    "            #g_loss = self.grad_loss(y, target)\n",
    "            #g_y \n",
    "            #gw3\n",
    "            #gb3\n",
    "            #gh2\n",
    "            #gw2\n",
    "            #gb2\n",
    "            #gh1\n",
    "            #gw1\n",
    "            #gb1\n",
    "            \n",
    "            # updating the weight\n",
    "        #    learning_rate = self.learning_rate * (self.decay_rate**i)\n",
    "        #    w3 -= learning_rate*gw3\n",
    "        #    b3 -= learning_rate*gb3\n",
    "        #    w2 -= learning_rate*gw2\n",
    "        #    b2 -= learning_rate*gb2\n",
    "        #    w1 -= learning_rate*gw1\n",
    "        #    b1 -= learning_rate*db1\n",
    "        #self.weights = (w1, w2, w3)\n",
    "        #self.bias = (b1, b2, b3)\n",
    "        #loss = self.loss(y, target)\n",
    "        #print('The final loss is {:12.4f} after {:4d} iterations'.format(self.loss, self.iterations))\n",
    "    \n",
    "            \n",
    "    def forward(self, X, weights, hiddenlayers):\n",
    "        \"\"\"Perform neural network forward pass.\"\"\"\n",
    "        output = {}\n",
    "        n_features = len(X)\n",
    "\n",
    "        nn_structure = [n_features] + hiddenlayers + [1]\n",
    "        nn_structure_len = len(nn_structure)\n",
    "\n",
    "        output[0] = X\n",
    "\n",
    "        for i in range(nn_structure_len-1):\n",
    "            output[i+1] = np.dot(weights[i], output[i])\n",
    "\n",
    "            if i < nn_structure_len-2:\n",
    "                output[i+1] = np.hstack(([1.],output[i+1]))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def activation(self, x, method='sigmoid'):\n",
    "        \"\"\"Activating the node\"\"\"\n",
    "        \n",
    "        allmethod = ('linear', 'sigmoid')\n",
    "        \n",
    "        if self.activation_method == 'linear':\n",
    "            activation = self.identity(x)\n",
    "        elif self.activation_method == 'sigmoid':\n",
    "            activation = self.sigmoid(x)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"The {method} is not implemented. Try from {allmethod}\")\n",
    "            \n",
    "        return activation\n",
    "    \n",
    "        \n",
    "    def init_random_weights(self, hiddenlayers, feature_length, seed=None):\n",
    "        \"\"\"Initializing random weights\"\"\"\n",
    "\n",
    "        rs = np.random.RandomState(seed=seed)\n",
    "\n",
    "        # Initialized weights and bias\n",
    "        weights = {}\n",
    "        nn_structure = [feature_length] + hiddenlayers + [1] #[input, hiddenlayers, output]\n",
    "        nn_structure_len = len(nn_structure)\n",
    "\n",
    "        for l in range(nn_structure_len-1):\n",
    "            epsilon = np.sqrt(6. / (nn_structure[l] + nn_structure[l+1]))\n",
    "            norm_epsilon = 2. * epsilon\n",
    "            \n",
    "            # (nn_structure[l+1], nn_structure[l] + 1) the size of the weight; plus 1 is for bias\n",
    "            weights[l] = rs.random_sample((nn_structure[l+1], nn_structure[l]+1)) * \\\n",
    "                         norm_epsilon - norm_epsilon / 2.\n",
    "\n",
    "        return weights\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        \"\"\"Compute the logistic sigmoid function.\"\"\"\n",
    "        \n",
    "        sigmoid = 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        if derivative:\n",
    "            return sigmoid * (1. - sigmoid)\n",
    "        else:\n",
    "            return sigmoid\n",
    "        \n",
    "        \n",
    "    def identity(self, x, derivative=False):\n",
    "        \"\"\"Compute the identity function\"\"\"\n",
    "        \n",
    "        if derivative:\n",
    "            pass\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    #################### Not implemented ##################\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        return predictions after training algorithm\n",
    "        \"\"\"\n",
    "        (h1, h2, y) = self.forward(X)\n",
    "        return y\n",
    "    \n",
    "    def grad_activation(self, x, method='sigmoid'):\n",
    "        if self.activation_method=='sigmoid':\n",
    "            return x*(1-x)\n",
    "        elif self.activation_method=='log_loss':\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "    def loss(self, y, target):\n",
    "        if self.loss_method=='mse':\n",
    "            return 0.5*np.sum(np.power(y-target,2))\n",
    "        elif self.loss.method=='log_loss':\n",
    "            pass\n",
    "        else:\n",
    "            raise Notimplementederror\n",
    "    \n",
    "    def grad_loss(self, y, target):\n",
    "        if self.loss_method=='mse':\n",
    "            return y-target\n",
    "        elif self.loss_method=='log_loss':\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain and preprocess the data\n",
    "from sklearn.datasets import load_wine\n",
    "data=load_wine()\n",
    "x, Y = data.data, data.target\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(x)  \n",
    "x0 = scaler.transform(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = my_MLPClassifier(hiddenlayers=[4,4])\n",
    "mlp.fit(x0, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46649148])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.output[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
